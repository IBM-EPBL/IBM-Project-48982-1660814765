{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3318a289",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"nbformat\": 4,\n",
    "  \"nbformat_minor\": 0,\n",
    "  \"metadata\": {\n",
    "    \"colab\": {\n",
    "      \"provenance\": []\n",
    "    },\n",
    "    \"kernelspec\": {\n",
    "      \"name\": \"python3\",\n",
    "      \"display_name\": \"Python 3\"\n",
    "    },\n",
    "    \"language_info\": {\n",
    "      \"name\": \"python\"\n",
    "    }\n",
    "  },\n",
    "  \"cells\": [\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": 2,\n",
    "      \"metadata\": {\n",
    "        \"id\": \"yhrkzc9ZpmOU\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"from keras.preprocessing.image import ImageDataGenerator\\n\",\n",
    "        \"train_datagen=ImageDataGenerator(rescale=1./255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)\\n\",\n",
    "        \"test_datagen=ImageDataGenerator(rescale=1./255)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"x_train = train_datagen.flow_from_directory('/content/Dataset/training_set',target_size=(64,64),batch_size=300,class_mode='categorical',color_mode=\\\"grayscale\\\")\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"911jlfgkNnqq\",\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"outputId\": \"44f120a6-86ad-47c5-c78a-b8329539d64c\"\n",
    "      },\n",
    "      \"execution_count\": 3,\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"output_type\": \"stream\",\n",
    "          \"name\": \"stdout\",\n",
    "          \"text\": [\n",
    "            \"Found 15750 images belonging to 9 classes.\\n\"\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"x_test = test_datagen.flow_from_directory('/content/Dataset/test_set',target_size=(64,64),batch_size=300,class_mode='categorical',color_mode=\\\"grayscale\\\")\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"id\": \"0jQeqRgJQ_k3\",\n",
    "        \"outputId\": \"d87d023d-a162-431d-e065-6886122187a2\"\n",
    "      },\n",
    "      \"execution_count\": 4,\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"output_type\": \"stream\",\n",
    "          \"name\": \"stdout\",\n",
    "          \"text\": [\n",
    "            \"Found 2250 images belonging to 9 classes.\\n\"\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"from keras.models import Sequential\\n\",\n",
    "        \"from keras.layers import Dense\\n\",\n",
    "        \"from keras.layers import Convolution2D\\n\",\n",
    "        \"from keras.layers import MaxPooling2D\\n\",\n",
    "        \"from keras.layers import Dropout\\n\",\n",
    "        \"from keras.layers import Flatten\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"hENrq9luV5CV\"\n",
    "      },\n",
    "      \"execution_count\": 5,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"model = Sequential()\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"_Xog4wM-WHQL\"\n",
    "      },\n",
    "      \"execution_count\": 6,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"model.add(Convolution2D(32,(3,3),input_shape=(64,64,1), activation='relu'))\\n\",\n",
    "        \"#no. of feature detectors, size of feature detector, image size, activation function\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"IxBznrvkXOiQ\"\n",
    "      },\n",
    "      \"execution_count\": 7,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"model.add(MaxPooling2D(pool_size=(2,2)))\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"m9i6nyiiYAzH\"\n",
    "      },\n",
    "      \"execution_count\": 8,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"model.add(Flatten())\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"YrEJW7pAYFA4\"\n",
    "      },\n",
    "      \"execution_count\": 9,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"model.add(Dense(units=512, activation = 'relu'))\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"qIvMupXlYg8d\"\n",
    "      },\n",
    "      \"execution_count\": 10,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"model.add(Dense(units=9,  activation = 'softmax'))\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"BSaehFfcY4iz\"\n",
    "      },\n",
    "      \"execution_count\": 11,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"Dq7W6q62Y9RC\"\n",
    "      },\n",
    "      \"execution_count\": 12,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"model.fit_generator(x_train,steps_per_epoch=24,epochs=10,validation_data = x_test, validation_steps= 40)\\n\",\n",
    "        \"#steps_per_epoch = no. of train images//batch size\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"id\": \"T530ZkC6ZSOk\",\n",
    "        \"outputId\": \"0adc68b9-1573-482b-87bd-fa1d56cb730f\"\n",
    "      },\n",
    "      \"execution_count\": 13,\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"output_type\": \"stream\",\n",
    "          \"name\": \"stderr\",\n",
    "          \"text\": [\n",
    "            \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\\n\",\n",
    "            \"  \\\"\\\"\\\"Entry point for launching an IPython kernel.\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"output_type\": \"stream\",\n",
    "          \"name\": \"stdout\",\n",
    "          \"text\": [\n",
    "            \"Epoch 1/10\\n\",\n",
    "            \"24/24 [==============================] - ETA: 0s - loss: 1.2714 - accuracy: 0.6219\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"output_type\": \"stream\",\n",
    "          \"name\": \"stderr\",\n",
    "          \"text\": [\n",
    "            \"WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 40 batches). You may need to use the repeat() function when building your dataset.\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"output_type\": \"stream\",\n",
    "          \"name\": \"stdout\",\n",
    "          \"text\": [\n",
    "            \"\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\b\\r24/24 [==============================] - 41s 2s/step - loss: 1.2714 - accuracy: 0.6219 - val_loss: 0.4031 - val_accuracy: 0.8982\\n\",\n",
    "            \"Epoch 2/10\\n\",\n",
    "            \"24/24 [==============================] - 33s 1s/step - loss: 0.2827 - accuracy: 0.9211\\n\",\n",
    "            \"Epoch 3/10\\n\",\n",
    "            \"24/24 [==============================] - 34s 1s/step - loss: 0.1448 - accuracy: 0.9615\\n\",\n",
    "            \"Epoch 4/10\\n\",\n",
    "            \"24/24 [==============================] - 32s 1s/step - loss: 0.0958 - accuracy: 0.9746\\n\",\n",
    "            \"Epoch 5/10\\n\",\n",
    "            \"24/24 [==============================] - 34s 1s/step - loss: 0.0679 - accuracy: 0.9826\\n\",\n",
    "            \"Epoch 6/10\\n\",\n",
    "            \"24/24 [==============================] - 32s 1s/step - loss: 0.0424 - accuracy: 0.9909\\n\",\n",
    "            \"Epoch 7/10\\n\",\n",
    "            \"24/24 [==============================] - 32s 1s/step - loss: 0.0373 - accuracy: 0.9908\\n\",\n",
    "            \"Epoch 8/10\\n\",\n",
    "            \"24/24 [==============================] - 33s 1s/step - loss: 0.0319 - accuracy: 0.9915\\n\",\n",
    "            \"Epoch 9/10\\n\",\n",
    "            \"24/24 [==============================] - 32s 1s/step - loss: 0.0235 - accuracy: 0.9940\\n\",\n",
    "            \"Epoch 10/10\\n\",\n",
    "            \"24/24 [==============================] - 32s 1s/step - loss: 0.0170 - accuracy: 0.9972\\n\"\n",
    "          ]\n",
    "        },\n",
    "        {\n",
    "          \"output_type\": \"execute_result\",\n",
    "          \"data\": {\n",
    "            \"text/plain\": [\n",
    "              \"<keras.callbacks.History at 0x7fe3bd2e8c90>\"\n",
    "            ]\n",
    "          },\n",
    "          \"metadata\": {},\n",
    "          \"execution_count\": 13\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"model.save('aslpng1.h5')\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"tbD4YC8VZlIB\"\n",
    "      },\n",
    "      \"execution_count\": 14,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"from keras.models import load_model\\n\",\n",
    "        \"import numpy as np\\n\",\n",
    "        \"import cv2\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"wBCEfO5qd0Gj\"\n",
    "      },\n",
    "      \"execution_count\": 17,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"model=load_model('aslpng1.h5')\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"sZYDfTiuZmUU\"\n",
    "      },\n",
    "      \"execution_count\": 18,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"from skimage.transform import resize\\n\",\n",
    "        \"def detect(frame):\\n\",\n",
    "        \"  img = resize(frame,(64,64,1))\\n\",\n",
    "        \"  img = np.expand_dims(img,axis=0)\\n\",\n",
    "        \"  if(np.max(img)>1):\\n\",\n",
    "        \"    img = img/255.0\\n\",\n",
    "        \"  prediction = model.predict(img)\\n\",\n",
    "        \"  print(prediction)\\n\",\n",
    "        \"  prediction = np.argmax(prediction,axis=1)\\n\",\n",
    "        \"  print(prediction)\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"05vEcPg2bJfW\"\n",
    "      },\n",
    "      \"execution_count\": 25,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"frame=cv2.imread('/content/Dataset/test_set/G/1.png')\\n\",\n",
    "        \"data = detect(frame)\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"id\": \"bdi1cEHebj35\",\n",
    "        \"outputId\": \"147bc04d-4759-43cd-90ab-fd53ee38317e\"\n",
    "      },\n",
    "      \"execution_count\": 26,\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"output_type\": \"stream\",\n",
    "          \"name\": \"stdout\",\n",
    "          \"text\": [\n",
    "            \"1/1 [==============================] - 0s 25ms/step\\n\",\n",
    "            \"[[2.9662006e-09 3.0511607e-09 5.7518361e-07 2.6636766e-09 7.6029876e-09\\n\",\n",
    "            \"  1.4324395e-08 9.9982303e-01 1.7639149e-04 1.6517550e-09]]\\n\",\n",
    "            \"[6]\\n\"\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"LQ1_aUY3b1qJ\"\n",
    "      },\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": []\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
